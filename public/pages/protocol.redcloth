This page describes the competition protocol in details. It also provides information about the "target machines":protocol#machines, the "training and test samples":protocol#training-set and the "evaluation":protocol#scoring of submitted test results.

h3(#setup). Competition set up

The competition server has a total of 100 <i>problems</i> - each problem is a sample of positive and negative strings generated by randomly walking a target machine. These problems vary in terms of their difficulty, related to 

* the <u>size of the alphabet</u> in the target machine 
* the <u>sparsity of the sample</u> (the extent to which it covers the behaviour of the target machine). 

The 100 problems are divided into <i>cells</i> of 5 problems each, where each cell corresponds to a particular combination of sparsity and alphabet size. The table below shows how the cells are organised. Easier problems (with a smaller alphabet and a larger sample) are towards the upper-left of the table, and the harder problems (larger alphabet and smaller sample) are towards the bottom-right.

!{grid_tools.abstract_grid("5 problems to solve in each cell", "left")}
!{grid_tools.points_grid("Cell points", "right")}
<div class="clear"></div>

Participation in the competition consists in attempting to <i>solve</i> cells. A cell is considered to be solved if all of the 5  problems it contains are solved with a @{protocol#scoring}{BCR score} of at least 0.99. Each cell is given a number of points (from 1 to 4) according to its difficulty, based on the average performance of the @{baseline}{Blue-Fringe algorithm}, computed with our @{protocol#scoring}{scoring procedure}. <u>The first technique to solve a hardest cell, among those solved in the competition, will be the winner</u>. The organizers will add additional problems if all cells appear to be solved more than one month before the final deadline.  

h3(#machines). Machines

The target machines are generated using a @{machines}{variant of the Forest-Fire algorithm} and present characteristics that are similar to state machines that model the behaviour of software systems:

- Number of states := State machines in the competition have <u>approximately 50 states</u>, which is representative of case studies found in the software engineering literature. A roughly <u>equal proportion of accepting and rejecting states</u> has been chosen, a feature shared with "Abbadingo":http://www-bcl.cs.may.ie/.

- Larger alphabets := State machine transitions are labelled by some alphabet symbol. Such a symbol may typically correspond to the input that triggers a transition or to a system function that governs the transition. For simplicity in this competition, alphabet symbols are represented by integer literals. Alphabet sizes in the competition range from 2 to 50 symbols (while previous competitions such as Abbadingo focussed on 2 letter alphabets)

- Degree distribution := State machines of software systems tend to contain a <u>small proportion of states with a high out-degree</u> while <u>most states have in- and out-degrees of one or two transitions</u>. Some states are sink accepting states, that is, an out-degree of 0.

- Deterministic and minimal := The machines considered in this competition are both <u>deterministic and minimal</u>. A machine is deterministic if, for all states, there is at most one outgoing transition on any alphabet symbol. A machine is minimal if no smaller deterministic machine defines the same regular language. 

@{machines}{Learn more} about target machine generation...

h3(#training-set). Training and test samples

For each problem, disjoint training and test samples have been generated using a @{samples}{dedicated generation procedure} whose aim is to simulate the way examples of system behaviour are usually obtained in the software engineering community. Samples present the following characteristics:

- Generated by the target := Positive strings have been generated by <u>randomly walking the target machines</u>. Negative strings are generated by <u>randomly perturbing positive strings</u>. Three kinds of edit operation are considered: substituting, inserting, or deleting a symbol.

- Training samples := Training sets are sampled with <u>different levels of sparsity</u> (100%, 50%, 25%, 12.5%) and usually <u>contain duplicates</u>, as a consequence of the random walk generation from the target machine.

- Test samples := Each test sample contains <u>1,500 strings</u> and contains <u>no duplicates</u> (to avoid favoring repeated strings in the scoring metric). <u>Training and test sets do not intersect</u>.

- Empirically adjusted := The different parameters of the generation procedure (the sample sizes, in particular) have all been empirically adjusted to ensure good induction results using Blue-Fringe on the simplest problems (alphabet of size 2 with a learning sample sparsity of 100%) without breaking the cell.

@{samples}{Learn more} about samples generation...

h3(#scoring). Scoring

Once the test set for each problem has been downloaded, it is up to the competitor to establish whether each test string is accepted or rejected by their candidate machine for that problem. The competitor should produce a binary sequence of labels where, for each test string, a 1 is added to the sequence if the string is considered to be accepted, and a 0 otherwise:

p=. <code>0100100101101010001011010011100101...</code> (up to the size of the test sample)

The score is computed according to a Balanced Classification Rate (<span class="math">_BCR_</span>) measure. The submitted binary sequence of labels is compared to the actual labels, and four sets are produced:

div(math). _TP_ : True positives
           _TN_ : True negatives
           _FP_ : False positives
           _FN_ : False negatives

These figures are used to compute two numbers, <span class="math"><em>C[^plus^]</em></span> and <span class="math"><em>C[^minus^]</em></span>

div(math). _C[^plus^]_&nbsp;&nbsp;= _TP_ / (_TP_ + _FN_) 
           _C[^minus^]_&nbsp;= _TN_ / (_TN_ + _FP_)

The BCR value is the harmonic mean between <span class="math"><em>C[^plus^]</em></span> and <span class="math"><em>C[^minus^]</em></span> and is defined as follows:

div(math). _BCR_ = (2 * <em>C[^plus^]</em> * <em>C[^minus^]</em>) / (<em>C[^plus^]</em> + <em>C[^minus^]</em>)

A _problem_ is deemed to be solved if the <span class="math">_BCR_</span> value is greater than or equal to 0.99. A _cell_ is deemed to be solved if all of its 5 problems are solved. Cells of the competition and participant result grids turn to green when solved.
